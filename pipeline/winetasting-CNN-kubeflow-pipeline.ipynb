{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --user --upgrade pip\n",
    "\n",
    "#!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy==1.2.1 scikit-learn==0.22 tensorflow==2.0 keras==1.2.2 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install kubeflow pipeline sdk\n",
    "#!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if installation was successful\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for pipeline\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "#output_dir = \"/home/jovyan/winetasting-groupf/data/\"\n",
    "output_dir = \"/home/jovyan/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessing fucntion\n",
    "\n",
    "def selection(data_path):\n",
    "    \n",
    "    # Import Libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'nltk==3.2.5'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'requests'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'zipfile'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import io\n",
    "    import tensorflow as tf\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from tensorflow.python import keras\n",
    "    import requests\n",
    "    import re\n",
    "    import zipfile\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "    #downloading the dataset\n",
    "    url1 = 'https://raw.github.com/HamoyeHQ/stage-f-06-wine-tasting/master/data/top_40_varieties.zip'\n",
    "    url2 = 'https://raw.github.com/HamoyeHQ/stage-f-06-wine-tasting/master/data/top_varieties_count.csv'\n",
    "    \n",
    "    #unzipping and reading data from url1\n",
    "    resp = requests.get(url1)\n",
    "    with zipfile.ZipFile(io.BytesIO(resp.content), 'r') as zf:\n",
    "        with zf.open('top_40_varieties.csv') as f:\n",
    "            top_40_varieties = pd.read_csv(f)\n",
    "            \n",
    "    #reading data from url2\n",
    "    top_varieties_count = pd.read_csv(url2)\n",
    "    \n",
    "    # replacing every occurence of 'US' in country with 'United States of America'\n",
    "    top_40_varieties['country'].replace('US', 'United States of America', inplace=True)\n",
    "\n",
    "    # replacing every occurence of 'US' in not_vintage with 'United States of America'\n",
    "    top_40_varieties['not_vintage'] = top_40_varieties['not_vintage'].apply(lambda x: x.replace(\\\n",
    "                                                                        'US', 'United States of America'))\n",
    "    # renaming the columns in top_varieties_count\n",
    "    top_varieties_count = top_varieties_count.rename(columns={'variety': 'count', 'Unnamed: 0': 'variety'})\n",
    "    top_varieties_count = top_varieties_count.set_index('variety') # setting the index\n",
    "    top_varieties_count = top_varieties_count['count'] # making it a Series\n",
    "    \n",
    "    top = 20 # selecting top 20 varities as our working varieties. note 1 < n <= 40\n",
    "\n",
    "    # making a datframe of our selecting top n varieties\n",
    "    top_df = top_40_varieties[top_40_varieties['variety'].isin(top_varieties_count.iloc[:top].index)]\n",
    "    \n",
    "    # threshold of miniority variety to over sample (use sentences as document instead of the whole description)\n",
    "    minority_threshold = 5000 \n",
    "\n",
    "    # making a dataframe of the miniority classes\n",
    "    minority_df = top_df[top_df['variety'].isin(top_varieties_count[top_varieties_count < \\\n",
    "                                                                          minority_threshold].index)]\n",
    "    \n",
    "    oversampled_miniority_lst = [] # empty list to store sentences as tokens miniority corpus\n",
    "\n",
    "    # creating a function to use sentences as tokens for the miniority classes\n",
    "    def over_sample_miniority(row):\n",
    "        doc_list = sent_tokenize(row['description'])\n",
    "        for sent in doc_list:\n",
    "            row['description'] = sent\n",
    "            oversampled_miniority_lst.append(list(row)) \n",
    "            \n",
    "    minority_df.apply(over_sample_miniority, axis=1); # over sample the miniority classes\n",
    "    \n",
    "    # converts oversampled_miniority_lst to a dataframe\n",
    "    oversampled_miniority_df = pd.DataFrame(oversampled_miniority_lst, columns=minority_df.columns)\n",
    "\n",
    "    # selecting majority classes as a dataframe to concatenate to oversampled_miniority_lst\n",
    "    majority_df = top_df[top_df['variety'].isin(\\\n",
    "                                            top_varieties_count[top_varieties_count >= minority_threshold].index)]\n",
    "\n",
    "    # concatenates majority_df to oversampled_miniority_lst\n",
    "    balanced_df = pd.concat([majority_df, oversampled_miniority_df]) \n",
    "    balanced_df = balanced_df.reset_index().drop('index', axis=1) # resets index\n",
    "    \n",
    "    balanced_variety = balanced_df['variety'].value_counts() # gets a Series of the variety count in balanced_df\n",
    "    \n",
    "    # for sentence oversampling\n",
    "    sent_oversample_corpus = [doc1 + ' ' + doc2 for doc1, doc2 in zip(\\\n",
    "                                                        balanced_df['description'], balanced_df['not_vintage'])]\n",
    "\n",
    "    labels = [label for label in balanced_df['variety']]\n",
    "\n",
    "    #Save the whole_data as a pickle file to be used by the preprocess component.\n",
    "    with open(f'{data_path}/selected_data', 'wb') as f:\n",
    "        pickle.dump((sent_oversample_corpus, labels), f)\n",
    "    \n",
    "    return (print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "selection(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path):\n",
    "    \n",
    "    #importing and installing libraries\n",
    "    # Import Libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy==2.2.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'dill'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==1.2.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'requests'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import dill\n",
    "    import spacy\n",
    "    import requests\n",
    "    import re\n",
    "    import time\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    # Load and unpack the selected_data\n",
    "    with open(f'{data_path}/selected_data','rb') as f:\n",
    "        selected_data = pickle.load(f)\n",
    "        \n",
    "    # Separate the independent data (X) from the dependent data(y).\n",
    "    X_select,  y_select = selected_data\n",
    "    \n",
    "    # creating a spacy pipeline and disabling tagger, parser and ner to speed up tokenizer\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner']) \n",
    "    \n",
    "    spacy_stop_words = spacy.lang.en.STOP_WORDS # getting spacy's stop-words\n",
    "    \n",
    "    # downloading yoast_stop_words to be included to spacy's stop words to improve performance\n",
    "    response = requests.get('https://raw.github.com/Yoast/YoastSEO.js/develop/src/config/stopwords.js')\n",
    "    yoast_stop_words = response.content.decode()\n",
    "    \n",
    "    pattern = r'\\[.+\\]'\n",
    "    match = re.search(pattern, yoast_stop_words)\n",
    "    yoast_stop_words = set(match.group()[1:-1].replace('\"', '').replace(',', '').split())\n",
    "    \n",
    "    stop_words_lemma = {word.lemma_.lower() for word in nlp(' '.join(spacy_stop_words | yoast_stop_words))} | \\\n",
    "            {'-pron-', '10', '12', 'aah', 'aa', 'ab', 'aaa', 'aand', '16', '2', '20', '30', '4', '40', '5', '6', '7', \\\n",
    "             '8', '9'}\n",
    "    \n",
    "    #creating custom transformers to encapsulate our preprocessing\n",
    "    class GetTokens(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, stop_words=stop_words_lemma):\n",
    "            self.stop_words = stop_words\n",
    "\n",
    "        # defining tokenzer function to tokenize the lower case lemma of documents in a corpus and \n",
    "        # filter out stop-words  \n",
    "        def tokenize(self, text):\n",
    "            return [word.lemma_.lower() for word in nlp(text) if word.is_alpha and word.lemma_.lower() not in self.stop_words]\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            self.tokens = [self.tokenize(doc) for doc in X]\n",
    "\n",
    "            return self.tokens\n",
    "    #GetTokens object\n",
    "    tokens = GetTokens()\n",
    "        \n",
    "    class Text2Sequence(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self):\n",
    "            self.sequence_tokenizer = Tokenizer(oov_token=-99)\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.sequence_tokenizer.fit_on_texts(X)\n",
    "            self.words_indices = self.sequence_tokenizer.word_index\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            self.get_sequences = self.sequence_tokenizer.texts_to_sequences(X)\n",
    "            return self.get_sequences\n",
    "    #text2sequence object\n",
    "    text_2_seq = Text2Sequence()\n",
    "    \n",
    "    class Padding(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, pad='post'):\n",
    "            self.pad = pad\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            self.get_paddings = pad_sequences(X, padding=self.pad)\n",
    "            return self.get_paddings\n",
    "    #padding object\n",
    "    pad = Padding()\n",
    "    \n",
    "    #building a pipeline for the transformers steps\n",
    "    data_prep_pipe = Pipeline([('get_tokens', tokens), ('text_2_sequence', text_2_seq), ('padding', pad)], verbose=1)\n",
    "    \n",
    "    #Save the data prep pipeline to be used by the train component.\n",
    "    with open(f'{data_path}/data_pipeline', 'wb') as f:\n",
    "        dill.dump((data_prep_pipe), f)\n",
    "    \n",
    "    return (print('Done!'))\n",
    "    \n",
    "    #return data_prep_pipe.fit_transform(X_select, y_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "preprocess(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training function\n",
    "\n",
    "def trainPredictCNN(data_path):\n",
    "    \n",
    "    # import Library\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','gensim'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'dill'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'requests'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==1.2.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy==2.2.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import time\n",
    "    import gzip\n",
    "    import requests\n",
    "    import re\n",
    "    from gensim.models import Word2Vec # importing Word2Vec\n",
    "    import dill\n",
    "    import spacy\n",
    "    from sklearn.utils import class_weight\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    # importing deep learning libraries\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from tensorflow.keras.layers import Embedding, Dense, GlobalMaxPool1D, Conv1D, Dropout\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    # creating a spacy pipeline and disabling tagger, parser and ner to speed up tokenizer\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner']) \n",
    "    \n",
    "    spacy_stop_words = spacy.lang.en.STOP_WORDS # getting spacy's stop-words\n",
    "    \n",
    "    # downloading yoast_stop_words to be included to spacy's stop words to improve performance\n",
    "    response = requests.get('https://raw.github.com/Yoast/YoastSEO.js/develop/src/config/stopwords.js')\n",
    "    yoast_stop_words = response.content.decode()\n",
    "    \n",
    "    pattern = r'\\[.+\\]'\n",
    "    match = re.search(pattern, yoast_stop_words)\n",
    "    yoast_stop_words = set(match.group()[1:-1].replace('\"', '').replace(',', '').split())\n",
    "    \n",
    "    stop_words_lemma = {word.lemma_.lower() for word in nlp(' '.join(spacy_stop_words | yoast_stop_words))} | \\\n",
    "            {'-pron-', '10', '12', 'aah', 'aa', 'ab', 'aaa', 'aand', '16', '2', '20', '30', '4', '40', '5', '6', '7', '8', '9'} \n",
    "    \n",
    "    # Load and unpack the selected_data\n",
    "    with open(f'{data_path}/selected_data','rb') as f:\n",
    "        selected_data = pickle.load(f)\n",
    "        \n",
    "    # Separate the independent data (X) from the dependent data(y).\n",
    "    X_select, y_select = selected_data\n",
    "    \n",
    "    # Load and unpack the data  pipeline\n",
    "    with open(f'{data_path}/data_pipeline','rb') as f:\n",
    "        data_pipeline = dill.load(f)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    one_hot = OneHotEncoder(sparse=False) \n",
    "        \n",
    "    encoded_labels = le.fit_transform(y_select)\n",
    "    one_hot_labels = one_hot.fit_transform(encoded_labels.reshape(-1, 1))\n",
    "    \n",
    "    \n",
    "    def get_embedding_matrix(model, word_index):\n",
    "        vocab_size = len(word_index) + 1\n",
    "        embedding_dim = model.wv.vector_size\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "        for word in model.wv.vocab:\n",
    "            ind = word_index[word]\n",
    "            embedding_matrix[ind] = model[word]\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def multi_class_fbeta(ytrue , ypred, beta=1, weighted=True, raw=False, epsilon=1e-7):\n",
    "        beta_squared = beta**2\n",
    "\n",
    "        ytrue = tf.cast(ytrue, tf.float32)\n",
    "        ypred= tf.cast(ypred, tf.float32)\n",
    "\n",
    "        max_prob = tf.reduce_max(ypred, axis=-1, keepdims=True)\n",
    "        ypred = tf.cast(tf.equal(ypred, max_prob), tf.float32)\n",
    "\n",
    "        tp = tf.reduce_sum(ytrue*ypred, axis=0)\n",
    "        predicted_positive = tf.reduce_sum(ypred, axis=0)\n",
    "        actual_positive = tf.reduce_sum(ytrue, axis=0)\n",
    "\n",
    "        precision = tp/(predicted_positive+epsilon)\n",
    "        recall = tp/(actual_positive+epsilon)\n",
    "\n",
    "        fb = (1+beta_squared)*precision*recall / (beta_squared*precision + recall + epsilon)\n",
    "\n",
    "        if raw:\n",
    "            return fb\n",
    "\n",
    "        if weighted:\n",
    "            supports = tf.reduce_sum(ytrue, axis=0)\n",
    "            return tf.reduce_sum(fb*supports / tf.reduce_sum(supports))\n",
    "\n",
    "        return tf.reduce_mean(fb)\n",
    "    \n",
    "    def build_cnn_model(embedding_matrix, input_length):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], \n",
    "                               weights=[embedding_matrix], \n",
    "                               input_length=input_length, \n",
    "                               trainable=False))\n",
    "\n",
    "        model.add(Conv1D(128, 3, activation='relu'))\n",
    "        model.add(Conv1D(128, 3, activation='relu'))\n",
    "\n",
    "        model.add(GlobalMaxPool1D())\n",
    "\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy', multi_class_fbeta])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    # setting class weights due to class imbalance\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.arange(20), encoded_labels)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    #general transformer to run lstm and cnn model\n",
    "    class NLPModel(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, build_fn, name, epochs=7, batch_size=128, verbose=0):\n",
    "            self.build_fn = build_fn\n",
    "            self.name = name\n",
    "            self.epochs = epochs\n",
    "            self.batch_size = batch_size\n",
    "            self.verbose = verbose\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            self.corpus = data_pipeline.named_steps['get_tokens'].tokens\n",
    "\n",
    "            t1 = time.time()\n",
    "            self.w2v_model = Word2Vec(self.corpus, size=300, min_count=1, iter=10)\n",
    "\n",
    "            print('Done training Word2Vec for {}                    total: {}mins'.format(self.name, \\\n",
    "                                                                              round((time.time()-t1)/60, 1)))\n",
    "\n",
    "            self.embedding_matrix = get_embedding_matrix(self.w2v_model, \\\n",
    "                                                         data_pipeline.named_steps['text_2_sequence'].words_indices)\n",
    "\n",
    "            self.model = self.build_fn(self.embedding_matrix, X.shape[1])\n",
    "\n",
    "            t2 = time.time()\n",
    "\n",
    "            self.history = self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, \\\n",
    "                                          class_weight=class_weights, verbose=self.verbose)\n",
    "\n",
    "            print('Done training {} model                           total: {}mins'.format(self.name, \\\n",
    "                                                                              round((time.time()-t2)/60, 1)))\n",
    "\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X):\n",
    "            self.pred = self.model.predict(X)\n",
    "            return self.pred\n",
    "        \n",
    "    #object of CNN\n",
    "    cnn_model = NLPModel(build_cnn_model, 'cnn_model', epochs=7) # instatiating cnn model object\n",
    "    \n",
    "    cnn_model_pipe = Pipeline([('data_prep', data_pipeline), ('model', cnn_model)])\n",
    "    \n",
    "    #traning the data\n",
    "    cnn_model_pipe.fit(X_select, encoded_labels)\n",
    "    \n",
    "    #predicting the data\n",
    "    y_pred = cnn_model_pipe.transform(X_select)\n",
    "    \n",
    "    # saving embedding matrix\n",
    "    with gzip.open(f'{data_path}/embedding_matrix.dill.gz', 'wb') as emb:\n",
    "        dill.dump(cnn_model.embedding_matrix, emb)\n",
    "        \n",
    "    # write predictions to results.txt\n",
    "    with open(f'{data_path}/CNNresults.txt','w') as result:\n",
    "        result.write(f'Prediciton: {le.inverse_transform(y_pred)[0]} | Actual {y_select}')\n",
    "    \n",
    "    print('CNN Prediction has be saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 3) Processing get_tokens, total= 1.3min\n",
      "[Pipeline] ... (step 2 of 3) Processing text_2_sequence, total=   5.6s\n",
      "[Pipeline] ........... (step 3 of 3) Processing padding, total=   1.1s\n",
      "Done training Word2Vec for cnn_model                    total: 1.2mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training cnn_model model                           total: 26.8mins\n",
      "CNN Prediction has be saved successfully!\n"
     ]
    }
   ],
   "source": [
    "trainPredictCNN(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and predict lightweight components.\n",
    "selection_op = comp.func_to_container_op(selection , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "preprocess_op = comp.func_to_container_op(preprocess , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "trainPredictCNN_op = comp.func_to_container_op(trainPredictCNN , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a client to enable communication with the Pipelines API server.\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='A wine tasting pipeline',\n",
    "   description='An ML pipeline that performs wine tasting model training and prediction.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def wine_tasting_pipeline(data_path: str):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)#RWO\n",
    "    \n",
    "    # Create winetasting selection component.\n",
    "    wine_tasting_selection_container = selection_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create winetasting preprocess component.\n",
    "    wine_tasting_preprocess_container = preprocess_op(data_path) \\\n",
    "                            .add_pvolumes({data_path: wine_tasting_selection_container.pvolume})\n",
    "\n",
    "    \n",
    "    # Create winetasting trainPredict component for both CNN and LSTM together.\n",
    "    wine_tasting_trainPredictCNN_container = trainPredictCNN_op(data_path) \\\n",
    "                            .add_pvolumes({data_path: wine_tasting_preprocess_container.pvolume})\n",
    "    \n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    wine_tasting_result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: wine_tasting_trainPredictCNN_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/CNNresults.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH =\"/home/jovyan/data\"\n",
    "#MODEL_PATH_CNN='cnn_model.hdf5'\n",
    "#MODEL_PATH_LSTM='lstm_model.hdf5'\n",
    "#MODEL_PATH_BLENDER='blender.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = wine_tasting_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/dsl/_container_op.py:1028: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/1baf4449-9d86-4475-8a5e-14c582a08c4b\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/e474a51f-c5ce-4381-9564-92ae1fceae98\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'wine_tasting_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH\n",
    "            }\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
